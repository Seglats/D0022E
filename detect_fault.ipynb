{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defect Classifier - Vision Transformer with Location Input\n",
    "Uses bounding box as input feature (like CNN model) rather than prediction target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATASET_PATH = Path('~/Documents/Git/D0022E/Data/images').expanduser()\n",
    "ANNOTATIONS_PATH = Path('~/Documents/Git/D0022E/Data/label').expanduser()\n",
    "\n",
    "CLASSES = ['crease', 'crescent_gap', 'inclusion', 'oil_spot', 'punching_hole', 'rolled_pit', 'silk_spot']\n",
    "\n",
    "# Model config\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "IN_CHANNELS = 1\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "MLP_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "LOC_DIM = 4  # cx, cy, w, h\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 60\n",
    "LR = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation(xml_path):\n",
    "    \"\"\"Parse XML and return location in center-width-height format (like CNN model).\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    size = root.find('size')\n",
    "    img_w = int(size.find('width').text)\n",
    "    img_h = int(size.find('height').text)\n",
    "    \n",
    "    obj = root.find('object')\n",
    "    if obj is None:\n",
    "        return None, None\n",
    "    \n",
    "    cls_name = obj.find('name').text\n",
    "    bbox = obj.find('bndbox')\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    xmin = int(bbox.find('xmin').text) / img_w\n",
    "    ymin = int(bbox.find('ymin').text) / img_h\n",
    "    xmax = int(bbox.find('xmax').text) / img_w\n",
    "    ymax = int(bbox.find('ymax').text) / img_h\n",
    "    \n",
    "    # Convert to center-width-height format (matching CNN model)\n",
    "    cx = (xmin + xmax) / 2\n",
    "    cy = (ymin + ymax) / 2\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    \n",
    "    return cls_name, [cx, cy, w, h]\n",
    "\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)).astype(np.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with location features\n",
    "X, y_cls, X_loc = [], [], []\n",
    "skipped = 0\n",
    "\n",
    "for cls in CLASSES:\n",
    "    cls_img_dir = DATASET_PATH / cls\n",
    "    cls_ann_dir = ANNOTATIONS_PATH / cls\n",
    "    \n",
    "    if not cls_img_dir.exists():\n",
    "        print(f'Missing image dir: {cls_img_dir}')\n",
    "        continue\n",
    "    \n",
    "    for img_path in cls_img_dir.glob('*.jpg'):\n",
    "        xml_path = cls_ann_dir / f'{img_path.stem}.xml'\n",
    "        \n",
    "        if not xml_path.exists():\n",
    "            xml_path = ANNOTATIONS_PATH / f'{img_path.stem}.xml'\n",
    "        \n",
    "        if not xml_path.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        img = preprocess_image(img_path)\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            ann_cls, loc = load_annotation(xml_path)\n",
    "            if ann_cls is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            X.append(img)\n",
    "            y_cls.append(cls)\n",
    "            X_loc.append(loc)\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "X = np.array(X)\n",
    "X_loc = np.array(X_loc, dtype=np.float32)\n",
    "y_cls = np.array(y_cls)\n",
    "\n",
    "print(f'Loaded: {len(X)}, Skipped: {skipped}')\n",
    "print(f'Location format: [cx, cy, w, h]')\n",
    "print(f'Location range: [{X_loc.min():.3f}, {X_loc.max():.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels and split\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_cls)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f'Classes: {label_encoder.classes_}')\n",
    "print(f'Distribution: {np.bincount(y_encoded)}')\n",
    "\n",
    "X_train, X_test, X_loc_train, X_loc_test, y_train, y_test = train_test_split(\n",
    "    X, X_loc, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f'\\nTrain: {len(X_train)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDataset(Dataset):\n",
    "    \"\"\"Dataset returning (image, location, label).\"\"\"\n",
    "    def __init__(self, images, locations, labels, augment=False):\n",
    "        self.images = torch.tensor(images, dtype=torch.float32).unsqueeze(1)\n",
    "        self.locations = torch.tensor(locations, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].clone()\n",
    "        loc = self.locations[idx].clone()  # [cx, cy, w, h]\n",
    "        \n",
    "        if self.augment:\n",
    "            # Horizontal flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                img = torch.flip(img, dims=[2])\n",
    "                loc[0] = 1 - loc[0]  # flip cx\n",
    "            \n",
    "            # Vertical flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                img = torch.flip(img, dims=[1])\n",
    "                loc[1] = 1 - loc[1]  # flip cy\n",
    "        \n",
    "        return img, loc, self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = DefectDataset(X_train, X_loc_train, y_train, augment=True)\n",
    "test_dataset = DefectDataset(X_test, X_loc_test, y_test, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerWithLocation(nn.Module):\n",
    "    \"\"\"ViT with location input branch (like CNN model).\n",
    "    \n",
    "    Location [cx, cy, w, h] is processed through MLP and concatenated\n",
    "    with ViT features before classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=1, num_classes=7,\n",
    "                 embed_dim=256, num_heads=8, num_layers=6, mlp_dim=512, \n",
    "                 dropout=0.1, loc_dim=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Location branch (mirrors CNN model structure)\n",
    "        self.loc_branch = nn.Sequential(\n",
    "            nn.Linear(loc_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classification head after concatenation\n",
    "        combined_dim = embed_dim + 64  # ViT features + location features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, x, loc):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Image branch (ViT)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        img_features = x[:, 0]  # CLS token\n",
    "        \n",
    "        # Location branch\n",
    "        loc_features = self.loc_branch(loc)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat([img_features, loc_features], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformerWithLocation(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    loc_dim=LOC_DIM\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "\n",
    "# Class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for images, locations, labels in loader:\n",
    "        images = images.to(device)\n",
    "        locations = locations.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, locations)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, locations, labels in loader:\n",
    "            images = images.to(device)\n",
    "            locations = locations.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images, locations)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_vit_location.pth')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} | '\n",
    "              f'Train: {train_loss:.4f}, {train_acc:.1%} | '\n",
    "              f'Val: {val_loss:.4f}, {val_acc:.1%}')\n",
    "\n",
    "print(f'\\nBest validation accuracy: {best_acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'], label='Val')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Val')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_title('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_vit_location.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, locations, labels in test_loader:\n",
    "        outputs = model(images.to(device), locations.to(device))\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, data, title in [(ax1, cm, 'Count'), (ax2, cm_norm, 'Normalized')]:\n",
    "    im = ax.imshow(data, cmap='Blues')\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_yticks(range(num_classes))\n",
    "    ax.set_xticklabels(label_encoder.classes_, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(label_encoder.classes_)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f'{title} - Acc: {(all_preds == all_labels).mean():.1%}')\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            val = f'{data[i,j]:.0f}' if title == 'Count' else f'{data[i,j]:.2f}'\n",
    "            color = 'white' if data[i,j] > data.max()/2 else 'black'\n",
    "            ax.text(j, i, val, ha='center', va='center', color=color, fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and config\n",
    "torch.save(model.state_dict(), 'defect_vit_location.pth')\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "config = {\n",
    "    'img_size': IMG_SIZE,\n",
    "    'patch_size': PATCH_SIZE,\n",
    "    'in_channels': IN_CHANNELS,\n",
    "    'num_classes': num_classes,\n",
    "    'embed_dim': EMBED_DIM,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'mlp_dim': MLP_DIM,\n",
    "    'dropout': DROPOUT,\n",
    "    'loc_dim': LOC_DIM\n",
    "}\n",
    "with open('model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, xml_path):\n",
    "    \"\"\"Predict class using image and location from XML.\"\"\"\n",
    "    img = preprocess_image(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    _, loc = load_annotation(xml_path)\n",
    "    if loc is None:\n",
    "        loc = [0.5, 0.5, 0.5, 0.5]  # default center\n",
    "    \n",
    "    img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    loc_tensor = torch.tensor(loc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor, loc_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred_idx = outputs.argmax(1).item()\n",
    "        confidence = probs[0, pred_idx].item()\n",
    "    \n",
    "    return {\n",
    "        'class': label_encoder.classes_[pred_idx],\n",
    "        'confidence': confidence,\n",
    "        'location': loc\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
